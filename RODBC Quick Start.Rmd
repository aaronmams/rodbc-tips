---
title: "RODBC Tips"
author: "aaron mamula"
date: "February 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,results="hide"}
#quick preamble
library(RODBC)
library(dplyr)
library(ggplot2)

```

## Prerequisites

1. Download/install the Oracle Client

2. Relocate the file 'tnsname.ora' to the directory, "C:/app/username/product/11.2.0/client_1/network/admin"

3. Set up a DSN

### Setting Up the ODBC Connection

Items 1 and 2 above are pretty straightforward.  Item 3 isn't trickey but does involve a few pointy-clicky steps.  [Here is a pretty good step-by-step resource for Windows users](https://kb.iu.edu/d/awqf)

I'm guessing most of us will have to have our IT people do this...but in case you are not yet constrained in that way here's the gist:

1. naviagate to: start->control panel->administrative tools->data sources (ODBC)
2. click 'add'
3. select the *Oracle in OraClient11g_home1* driver.
4. fill in the text boxes and hopefully the **TNS Service Name** drop down autopopulates with **PACFIN**
5. test connection

![data source administrator](R:/AaronMamula/PacFIN_Oracle/rodbc-tips/images/DataSources1.png)


![Add new DSN](R:/AaronMamula/PacFIN_Oracle/rodbc-tips/images/DataSources2.png)

![Test Connection](R:/AaronMamula/PacFIN_Oracle/rodbc-tips/images/DataSources3.png)


## Establish the PacFIN Connection in R

### Install and load the RODBC package

```{r}
#install.packages('RODBC')
library('RODBC')
sessionInfo()
```

RODBC_1.3-13 is attached so we are ready to rock.

### Connect to PacFIN

```{r}
#don't worry about this first part...I just need to get my PacFIN credentials in here without
# making them visiable
pf.cred <- read.csv("pflogon.csv")

#Identify ODBC connections
odbcDataSources(type=c('all'))

#get basic connection info:
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
odbcGetInfo(channel)
close(channel)

#check out what tables I can access
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sqlTables(channel,schema="PACFIN_MARTS")
close(channel)

#I can't access anything from NORPAC...or maybe there's just nothing in there
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sqlTables(channel,schema="NORPAC")
close(channel)


#the comprehensive tables have waaaaaaaaaay too many columns to reasonably display so I'm switching over
# to the Trawl Logbook FTID table here
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sqlColumns(channel,"LBK_FTID",schema="PACFIN")
close(channel)

```

## Do Some Stuff with Fish Tickets

### IFQ Sablefish Landings

The really convenient thing about the RODBC package is that you simply pass standard SQL queries as a character string and the query executes and returns results in an R data frame.

Here, I'm going to pull IFQ Sablefish landings from the **COMPREHENSIVE_FT** table from the **PACFIN_MARTS** schema:

```{r}
#get total landings of IFQ sablefish and aggregate by year, major port area, and gear type

channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sabl <- sqlQuery(channel,"select PACFIN_PORT_CODE,  LANDING_YEAR, PACFIN_GROUP_GEAR_CODE, sum(LANDED_WEIGHT_LBS)
                 from PACFIN_MARTS.COMPREHENSIVE_FT
                 where IS_IFQ_LANDING='T' and PACFIN_PORT_CODE in ('MRO','AVL','MNT','MOS','PRN','SF')
                 group by PACFIN_PORT_CODE, LANDING_YEAR, PACFIN_GROUP_GEAR_CODE")
close(channel)

sabl <- tbl_df(sabl)
sabl
names(sabl) <- c('port','year','gear','lbs')

#form a port aggregate and weed out some trivial gear types
sabl.cc <- sabl %>% filter(gear %in% c('HKL','TWL','POT')) %>% 
          mutate(portgroup=ifelse(port %in% c('MRO','AVL'),'Morro Bay',
                                  ifelse(port %in% c('MNT','MOS'),'Monterey Bay',
                                         ifelse(port=='PRN', 'Half Moon Bay','San Francisco')))) %>%
          group_by(portgroup,gear,year) %>%
          summarise(lbs=sum(lbs,na.rm=T)) %>%
            ungroup() %>%
          mutate(portgroup=factor(portgroup,levels=c('San Francisco','Half Moon Bay','Monterey Bay','Morro Bay'))) %>%
          mutate(gear=factor(gear,levels=c('HKL','POT','TWL')))

#plot CA Central Coast 
ggplot(sabl.cc,
       aes(x=year,y=lbs,fill=gear,order=gear)) + 
  geom_bar(stat='identity',position='stack') + facet_wrap(~portgroup) +
  theme_bw()

```


A few quick comments:

* *believeNRows=FALSE* I have no idea if/when this option is necessary to invoke.  Frankly, trial-and-error have convinced me it's better to include this than not to.  I have found that, without this option, my query will occassionally crap out around ~200,000 rows.  
* the *sqlQuery* function used above has an option *max* which, if set to 0, indicates no limit on the number of rows to be imported.  My experience has been that this option has no been effective at addressing the problem mentioned in the previous bullet.


### Alternative Sablefish IFQ Query

Depending on your text editor/text options within R Studio you may find that R interprets line breaks within an SQL Query in a funky way.  My experience has been that *sqlQuery* is pretty resilient to formatting of the text string that you pass...but I have occassionally found that queries spanning multiple lines get passed with "/n" tags...when that happens I just use R's *paste* function to solve the problem:

```{r}

paste("select PACFIN_PORT_CODE,  LANDING_YEAR, PACFIN_GROUP_GEAR_CODE, sum(LANDED_WEIGHT_LBS)
                 from PACFIN_MARTS.COMPREHENSIVE_FT
      where IS_IFQ_LANDING='T' and PACFIN_PORT_CODE in ('MRO','AVL','MNT','MOS','PRN','SF')
      group by PACFIN_PORT_CODE, LANDING_YEAR, PACFIN_GROUP_GEAR_CODE")

#those "/n" tags could mess with thing...but we can write our query over multiple lines without the tags
# like so:

q <- paste("select PACFIN_PORT_CODE, LANDING_YEAR, PACFIN_GROUP_GEAR_CODE",",",
           "sum(LANDED_WEIGHT_LBS)",
           "from PACFIN_MARTS.COMPREHENSIVE_FT",
           "where IS_IFQ_LANDING='T' and",
           "PACFIN_PORT_CODE in ('MRO','AVL','MNT','MOS','PRN','SF')",
           "group by PACFIN_PORT_CODE, LANDING_YEAR, PACFIN_GROUP_GEAR_CODE")

q

channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sabl.new <- sqlQuery(channel,q)
close(channel)
head(sabl.new)
```

### A Query with Dates

The **COMPREHENSIVE_FT** table conveniently has dates decomposed into month and year...but if you wanted to access part of a table that does have decomposed dates and you wanted to filter by particular dates you can use Oracles *EXTRACT* function for date objects

```{r}
# A Query with dates...

#the Oracle equivilent of 'DATEPART'
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sqlQuery(channel,"select count(FTID) from PACFIN_MARTS.COMPREHENSIVE_FT 
where extract(year from LANDING_DATE) > 2015")
close(channel)

#The query above should be the same as this:
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sqlQuery(channel,"select count(FTID) from PACFIN_MARTS.COMPREHENSIVE_FT 
         where LANDING_YEAR > 2015")
close(channel)

```


### Calculate Total Distance (Km) of Groundfish Trawl Trips

Note: this is pretty back-of-the-envelop...I only use trawl sets because the trawl retrieval coordinates are pretty patchy.

```{r}
# Logbook trip distances

#lat/long coordinates for each port and the key to match
# port identifiers between observer data and PacFIN data
port_codes <- read.csv("data/lbk_port_codes.csv")


#set-up port coordinates - we set this up by departure and
# return port because we will need lat/long for both in order
# to approximate the trip distance

dport_codes <- data.frame(d_port=port_codes$OBS_port,dport_lat=port_codes$LAT,dport_long=port_codes$LONG)
dport_codes <- dport_codes[!duplicated(dport_codes),]
dport_codes <- dport_codes[which(dport_codes$d_port!=""),]

rport_codes <- data.frame(r_port=port_codes$OBS_port,rport_lat=port_codes$LAT,rport_long=port_codes$LONG,rPCID=port_codes$PCID)
rport_codes <- rport_codes[!duplicated(rport_codes),]
rport_codes <- rport_codes[which(rport_codes$r_port!=""),]


#pull the tow level data from logbooks and join with logbook trip info
channel <- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
tows <- sqlQuery(channel,"select LBK_TOW.TRIP_ID, LBK_TOW.TOWNUM, LBK_TOW.SET_LAT, LBK_TOW.SET_LONG, LBK_TOW.AGID, LBK_TRIP.RPORT, LBK_TRIP.DPORT from PACFIN.LBK_TOW inner join PACFIN.LBK_TRIP on LBK_TOW.TRIP_ID=LBK_TRIP.TRIP_ID")
close(channel)

#fix longitude values
tows <- tbl_df(tows) %>% mutate(SET_LONG = SET_LONG * -1)

#-------------------------------------------------------------------------------------------------------------
#before joining the port codes data frame with the tows data frame we have to fix the Astoria code because it's
# '02' which often gets read as 2:
port_codes$LBK_PORT <- as.character(port_codes$LBK_PORT)
port_codes$LBK_PORT[which(port_codes$AGID=='O' & port_codes$PCID=='AST')] <- '02'

#------------------------------------------------------------------------------------------------------------


d_port_codes <- port_codes[,c("LBK_PORT","AGID","LAT","LONG")]
names(d_port_codes) <- c("DPORT","AGID","LAT","LONG")

tows <- tows %>% left_join(d_port_codes,by=c('DPORT','AGID'))
names(tows) <- c('TRIP_ID','TOWNUM','SET_LAT','SET_LONG','AGID','RPORT','DPORT','DPORT_LAT','DPORT_LONG')

r_port_codes <- port_codes[,c("LBK_PORT","AGID","LAT","LONG")]
names(r_port_codes) <- c("RPORT","AGID","LAT","LONG")

tows <- tows %>% left_join(r_port_codes,by=c('RPORT','AGID'))
names(tows) <- c('TRIP_ID','TOWNUM','SET_LAT','SET_LONG','AGID','RPORT','DPORT',
                 'DPORT_LAT','DPORT_LONG','RPORT_LAT','RPORT_LONG')

#------------------------------------------------------------------------------
#calculate distance per trip by connecting tow sets

#first a diagnostic...number of trips with no good tow locations and number of 
# trips with number of good tow set locations = number of tows
tows.tmp <- tows %>% group_by(TRIP_ID) %>% mutate(bad_pos=ifelse(is.na(SET_LAT),1,0),good_pos=1-bad_pos) %>%
  summarise(townum=max(TOWNUM),good_pos=sum(good_pos),bad_pos=sum(bad_pos))

tows.tmp %>% mutate(all.good=ifelse(good_pos==townum,1,0)) %>% ungroup() %>% summarise(all.good=sum(all.good))

length(tows.tmp$good_pos[tows.tmp$good_pos==0])

#Haversine distance between 2 sets of lat/long coordinates
dtemp.fn <- function(x,y){
  p1 <- (x*pi)/180
  p2 <- (y*pi)/180
  a <- sin((p2[1]-p1[1])*0.5)*sin((p2[1]-p1[1])*0.5) + sin((p2[2]-p1[2])*0.5)*sin((p2[2]-p1[2])*0.5)*cos(p1[1])*cos(p2[1])
  c <- 2*atan2(sqrt(a),sqrt(1-a))
  d <- c*6371
  return(d)
}


#filter the data set to include only good tow set locations
tow.locs <- tows %>% filter(!is.na(SET_LAT)) %>% filter(!is.na(SET_LONG)) %>% filter(!is.na(DPORT_LAT)) %>%
  filter(!is.na(RPORT_LAT)) %>% filter(!is.na(DPORT_LONG)) %>% filter(!is.na(RPORT_LONG)) %>%
  filter(SET_LAT > 20 & SET_LONG < 20) %>% 
  arrange(TRIP_ID,TOWNUM) 

#get the distance from departure port to first tow set
dstart <- tow.locs %>% arrange(TRIP_ID,TOWNUM) %>% group_by(TRIP_ID) %>% filter(row_number()==1) %>%
  mutate(dstart=dtemp.fn(x=c(DPORT_LONG,DPORT_LAT),y=c(SET_LONG,SET_LAT)))

quantile(dstart$dstart,probs=c(0.001,0.25,0.5,0.75,0.99))

# get distance from last tow to return port
dend <- tow.locs %>% arrange(TRIP_ID,TOWNUM) %>% group_by(TRIP_ID) %>% filter(row_number()==n()) %>%
  mutate(dend=dtemp.fn(x=c(RPORT_LONG,RPORT_LAT),y=c(SET_LONG,SET_LAT)))

# get distance between all tow sets on the trip...in order to do this one we need the trip to have
# at least two good tow set positions...for some reason this doesn't like my user defined function so
# we'll just hardcode the haversine distance
tow.dist <- tow.locs %>% select(TRIP_ID, TOWNUM, SET_LAT, SET_LONG, AGID, DPORT, RPORT) %>%
  group_by(TRIP_ID) %>% 
  arrange(TRIP_ID, TOWNUM) %>%
  mutate(ntows=n_distinct(TOWNUM),
         lat.now=(SET_LAT*pi)/180, 
         long.now=(SET_LONG*pi)/180,
         lag.long=(lag(SET_LONG)*pi)/180,
         lag.lat=(lag(SET_LAT)*pi/180)) %>%
  mutate(a=sin((long.now-lag.long)*0.5)*sin((long.now-lag.long)*0.5) + sin((lat.now-lag.lat)*0.5)*sin((lat.now-lag.lat)*0.5)*cos(lag.long)*cos(long.now),
         c= 2*atan2(sqrt(a),sqrt(1-a)),
         d=c*6371) %>%
  summarise(d=sum(d,na.rm=T))

#put the distances together
d <- dstart %>% select(TRIP_ID,dstart) %>% 
  inner_join(dend,by=c('TRIP_ID')) 
d <- d %>% 
  inner_join(tow.dist,by=c('TRIP_ID')) %>%
  mutate(trip.dist=dstart+dend+d)

#quick plot just to see how things look
plot.df <- d %>% filter(DPORT %in% c('AST','02','223')) %>% 
            mutate(DPORT.new = ifelse(DPORT %in% c('AST','02'),'Astoria',
                                      'Fort Bragg'))
ggplot(plot.df,aes(x=trip.dist)) + geom_histogram(color='black',fill='blue',alpha=0.3) + 
  facet_wrap(~DPORT.new) + theme_bw()
```

## Bonus

### Creating Tables
```{r}
# Creating tables

channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sqlTables(channel,schema="AMAMULA")
sqlQuery(channel, "CREATE TABLE comp_or_ifq AS
         (SELECT pacfin_year ,landing_month ,pacfin_gear_code ,pacfin_gear_description ,ifq_management_area ,pacfin_species_code ,pacfin_species_common_name ,SUM(round_weight_mtons) AS rnd_mtons
         FROM pacfin_marts.comprehensive_ft
         WHERE pacfin_year = 2016
         AND agency_code = 'O'
         AND is_ifq_landing = 'T'
         GROUP BY pacfin_year ,landing_month ,pacfin_gear_code ,pacfin_gear_description ,ifq_management_area ,pacfin_species_code ,pacfin_species_common_name);" )

sqlTables(channel,schema="AMAMULA")

comp_or_ifq <- sqlQuery(channel, "select * from AMAMULA.comp_or_ifq")
head(comp_or_ifq)

sqlQuery(channel, "DROP TABLE comp_or_ifq PURGE;")
close(channel)

#Create a View
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
sqlTables(channel,schema="AMAMULA")
sqlQuery(channel, "CREATE OR REPLACE VIEW comp_or_ifq_v AS
(SELECT pacfin_year ,landing_month ,pacfin_gear_code ,pacfin_gear_description ,ifq_management_area ,pacfin_species_code ,pacfin_species_common_name ,SUM(round_weight_mtons) AS rnd_mtons
FROM pacfin_marts.comprehensive_ft
WHERE pacfin_year = 2016
AND agency_code = 'O'
AND is_ifq_landing = 'T'
GROUP BY pacfin_year ,landing_month ,pacfin_gear_code ,pacfin_gear_description ,ifq_management_area ,pacfin_species_code ,pacfin_species_common_name);" )
sqlTables(channel,schema="AMAMULA")

 
sqlQuery(channel, "DROP VIEW comp_or_ifq_v;")
sqlTables(channel,schema="AMAMULA")
close(channel)

```

### Data Types and Stuff

This is something I rarely worry about because I haven't yet had a reason to worry much about it...but if you are somebody that needs to care a lot about data types you can use **sqlColumns** to describe the fields in a database...but honestly I've found it way easier to use SQL Developer for stuff like looking at database tables.

I usually use SQL Developer to do simple stuff like

* look at fields in a database table
* use the schema browser to see where various tables live
* run quick throw-away queries to find things like the unique values in a column

Here's a quick use-case: suppose I want to pull individual fish tickets from PacFIN but I'm only interested in a few gear types.  It would be pretty straightforward to just do a select * query then use R to filter for the stuff you want.  R can handle large amounts of data perfectly fine but if you're only interested in a few millions rows it doesn't make a ton of sense to pull 15 million rows into the workspace then filter it...better to filter it inside the query.  

Here's a comparison:

#### Option A: pull all the fish tickets from 2013 to 2015, examine, filter in R

```{r}
t <- Sys.time()
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
ft <- sqlQuery(channel, "select FTID, PACFIN_GEAR_CODE, PACFIN_SPECIES_CODE, LANDED_WEIGHT_LBS from PACFIN_MARTs.COMPREHENSIVE_FT where LANDING_YEAR > 2013")
close(channel)
Sys.time() - t

#get unique gear codes
unique(ft$PACFIN_GEAR_CODE)

#filter data frame
ft <- tbl_df(ft) %>% filter(!PACFIN_GEAR_CODE %in% c('RLT','BTR','MDT','TRL'))
```

Note that the query took about 43 seconds and I only got a few columns.

#### Option B: Use SQL Developer to get unique gear codes and pull only ones I want into R

Step 1: Open SQL Developer and look at unique gear codes

![Open SQL Developer](R:/AaronMamula/PacFIN_Oracle/rodbc-tips/images/sqldev1.png)

I'm sure there are many ways to enter invoke the password prompt but I usually right-click the PacFIN database icon in the 'Connections' options box then select 'Schema Browser.'  This brings up a dialog box where the PacFIN username and password can be entered.

![Query Unique Gear Types](R:/AaronMamula/PacFIN_Oracle/rodbc-tips/images/sqldev2.png)

I've never used anything other than the basic query builder in SQL Developer. I'm sure you can probably do some cool stuff with it...but for me it's just a tool to help refine some queries that I will ultimately just pipe through R.


Step 2: Pass the gear code filter inside the SQL query

```{r}
t <- Sys.time()
channel<- odbcConnect(dsn="pacfin",uid=paste(pf.cred$UID),pw=paste(pf.cred$PW),believeNRows=FALSE)
ft <- sqlQuery(channel, "select FTID, PACFIN_GEAR_CODE, PACFIN_SPECIES_CODE, LANDED_WEIGHT_LBS 
               from PACFIN_MARTs.COMPREHENSIVE_FT where LANDING_YEAR > 2013 and PACFIN_GEAR_CODE in ('RLT','BTR','MDT','TRL')")
close(channel)
Sys.time() - t

```

Firing up the SQL Developer adds an extra step but can save a lot of time if you are pulling from a big table like the fish tickets.

### Maps

This is not intimately related to pull data from PacFIN but it's something I just started working that results in some cool pictures...so I included it.  I totally pirated this example from a [blog post I found here](http://www.kevjohnson.org/making-maps-in-r/).

R has a couple libraries that play nice with shapefiles which make R a more powerful GIS tool than a lot of people realize.  In the chunk below I followed a couple simple steps to create a pretty decent poverty map for California:

* pulled California Census Tract Boundaries from the Census Bureau's [TIGER Database](https://www.census.gov/geo/maps-data/data/cbf/cbf_tracts.html)
* used the Census Bureaus American Communities Survey to get families with dependent children below the poverty line organized by Census Tract
* used *fortify()* from the **ggplot2** package to covert the Census Tract Boundaries from a SpatialPolygonDataFrame object to a dataframe object
* used **dplyr**'s *left_join* function to join this data frame with the poverty data
* pulled California County Boundaries from the TIGER Database
* displayed the poverty percentages by Census Tract in the **ggplot** environment
* overlaid the County Boundaries instead of the Census Tract Boundaries just to make the map look nicer

```{r}
library(rgdal)
library(rgeos)
library(maptools)
library(scales)
library(dplyr)
library(ggplot2)
library(mapproj)

ca.tract <- readOGR(dsn = "data", layer = "cb_2015_06_tract_500k")
ca.tract <- fortify(ca.tract,region="AFFGEOID")

ca.poverty <- read.csv("data/acs_poverty_family.csv")
ca.poverty <- ca.poverty[,c('GEO.id','allfamilies_depchildren_pctpoverty')]
names(ca.poverty) <- c('id','pct_poverty')

plotData <- left_join(ca.tract,ca.poverty)

#add county boundaries so the map doesn't get too busy
ca.county <- readOGR(dsn="data",layer="gz_2010_06_060_00_500k")
ca.county <- fortify(ca.county,region="COUNTY")

#converty values to decimal format just to play nice with the distiller
plotData$pct_poverty <- plotData$pct_poverty/100

#now map it and make it look nicer
ggplot() +
  geom_polygon(data = plotData, aes(x = long, y = lat, group = group,
                                    fill = pct_poverty)) +
  geom_polygon(data = ca.county, aes(x = long, y = lat, group = group),
               fill = NA, color = "black", size = 0.25) +
  coord_map() +
  scale_fill_distiller(palette = "Greens", labels = percent,
                       breaks = pretty_breaks(n = 10)) +
  guides(fill = guide_legend(reverse = TRUE)) + theme_bw()
```

This isn't totally unrelated to PacFIN.  Some of you heard about an ongoing project with PacFIN to get community level economic and demographic data into a PacFIN database.  This will likely make it much easier for us to do community-level economic impact analysis because, rather than battle with the Census Bureau's datamart to get hundreds of data series saved as .csv files, we will be able to pull them from PacFIN.    
